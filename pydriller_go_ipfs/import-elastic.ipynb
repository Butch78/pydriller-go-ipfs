{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb50353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydriller\n",
    "from pydriller import ModificationType\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import streaming_bulk\n",
    "import json\n",
    "from datetime import date, datetime\n",
    "import tqdm\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10600c15",
   "metadata": {},
   "source": [
    "## Create the elasticsearch and pydriller clients\n",
    "\n",
    "First we instantiate the elasticsearch client. Without specifying parameters, it will look for an elasticsearch instance on localhost, port 9200 (default)\n",
    "\n",
    "We then specify the name of the index where we want to store our commit data. In this case, we will look at the elasticsearch git repository itself, so we call the index \"elasticsearch-commits\" but this is completely arbitrary.\n",
    "In case the index already exist and we want to wipe it to re-ingest the data we can run the delete command on the indeces.\n",
    "\n",
    "Finally, we create a pydriller repository object by specifing the location on the local file system where we cloned the desired repository, in our case \"C:\\Projects\\External-Examples\\go-ipfs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "298b1c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esclient = Elasticsearch()\n",
    "index_name = \"go-ipfs-commits-complete\"\n",
    "start_tag =  'v0.5.0'\n",
    "end_tag = 'v0.10.0'\n",
    "file_type = '.go'\n",
    "\n",
    "\n",
    "esrepo = pydriller.Repository('C:\\Projects\\External-Examples\\go-ipfs', from_tag=start_tag, to_tag=end_tag)\n",
    "\n",
    "# in case we want to wipe the index and re-ingest the data\n",
    "# esclient.indices.delete(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc67bf",
   "metadata": {},
   "source": [
    "## Define the elasticsearch schema\n",
    "\n",
    "In case this is the first time we import the data, or if we deleted the index, the next step is to specify the schema of the elasticsearch index we use. Note that we can also import the data without specifying the schema, and elastic will infer the schema itself. However, elastic doesn't know how we want to analyze the data, so using the default schema inference is not recommended.\n",
    "\n",
    "In the schema, besides the standard data type mapping such as integer, float, and datetime, which are simply a 1:1 mapping with python, we have text and keyword. keyword should be used for strings that we want to analyze as they are (for example author name or email, commit hash); text is for strings where we want to apply full text search capabilities. In this case we can also specify which analyzer to use, to define which stemming rules to apply and stop-world to remove. We use the english analyzer for the commit text messages, which are the only human writte piece of information in the commit (excluding the actual diff). In the mapping for the commit message (field msg) we also set \"fielddata\" to be true, so that we can apply techniques normally available only for keyword fields to the commit message (for example to generate the tag cloud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dcb8078d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'go-ipfs-commits-complete'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esclient.indices.create(\n",
    "    index_name, \n",
    "    body={\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"author.name\": {\"type\": \"keyword\"},\n",
    "                \"author.email\": {\"type\": \"keyword\"},\n",
    "                \"committer.name\": {\"type\": \"keyword\"},\n",
    "                \"committer.email\": {\"type\": \"keyword\"},\n",
    "                \"deletions\": {\"type\": \"integer\"},\n",
    "                \"insertions\": {\"type\": \"integer\"},\n",
    "                \"lines\": {\"type\": \"integer\"},\n",
    "                \"files\": {\"type\": \"integer\"},\n",
    "                \n",
    "                \"author_timezone\": {\"type\": \"integer\"}, \n",
    "                \"committer_date\": {\"type\": \"date\"},\n",
    "                \"committer_timezone\": {\"type\": \"integer\"}, \n",
    "                \"parents\": {\"type\": \"keyword\"},\n",
    "\n",
    "                \"old_path\": {\"type\": \"keyword\"},\n",
    "                \"new_path\": {\"type\": \"keyword\"},\n",
    "                \"filename\": {\"type\": \"keyword\"},\n",
    "                \"change_type\": {\"type\": \"keyword\"},\n",
    "                \"diff\": {\"type\": \"text\"},\n",
    "                \"added_lines\": {\"type\": \"integer\"},\n",
    "                \"deleted_lines\": {\"type\": \"integer\"},\n",
    "                \"source_code\": {\"type\": \"text\"},\n",
    "                \"source_code_before\": {\"type\": \"text\"},\n",
    "                \"methods\": {\"type\": \"text\"},\n",
    "                \"methods_before\": {\"type\": \"text\"},\n",
    "                \"changed_methods\": {\"type\": \"text\"},\n",
    "                \"nloc\": {\"type\": \"integer\"},\n",
    "                \"complexity\": {\"type\": \"integer\"},\n",
    "                \"token_count\": {\"type\": \"integer\"},\n",
    "            }}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2d1e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class modified_files(BaseModel):\n",
    "    complexity: Optional[int]\n",
    "    nloc: Optional[int]\n",
    "    old_path: Optional[str]\n",
    "    new_path: Optional[str]\n",
    "    change_type: Optional[ModificationType]\n",
    "    filename: Optional[str]\n",
    "    diff: Optional[str]\n",
    "    added_lines: Optional[int]\n",
    "    deleted_lines: Optional[int]\n",
    "    source_code: Optional[str]\n",
    "    source_code_before: Optional[str]\n",
    "    methods: Optional[List]\n",
    "    methods_before: Optional[List]\n",
    "    changed_methods: Optional[List]\n",
    "    token_count: Optional[int]\n",
    "    \n",
    "\n",
    "\n",
    "class commit(BaseModel):\n",
    "    hash: str\n",
    "    msg: str\n",
    "    author_date: datetime\n",
    "    author_name: str\n",
    "    author_email: str\n",
    "    modified_files: List[modified_files]\n",
    "    commiter_name: str\n",
    "    commiter_email: str\n",
    "    deletions: int\n",
    "    insertions: int\n",
    "    lines: int\n",
    "    files: int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce46f0",
   "metadata": {},
   "source": [
    "## Importing the data in ElasticSearch\n",
    "\n",
    "### Converting Pydantic Pydriller objects into json\n",
    "\n",
    "Once we have an elasticsearch index ready to receive our data, we need a way to convert Pydanitc pydriller objects into json format, the one expected by elasticsearch (we could also use other format with elasticsearch pipelines and plugins, but json is the default, easiest, and preferred format).\n",
    "\n",
    "To convert python objects into json, the standard way is to use the json library with the json.dumps function. We use the Fastapi's json_encode to convert the python objects into json.\n",
    "\n",
    "### Bulk importing the data\n",
    "\n",
    "Now that we have the index, the schema, and a way to encode pydriller objects into json, we are ready to import the data in ElasticSearch. To do that, we use the bulk endpoints, which are way more efficient than the standard ingest endpoints to import large number of documents.\n",
    "\n",
    "We first define a function that given a number of commits to import (`limit`), traverses the git history, converts every commit into json and yields back the converted commit.\n",
    "\n",
    "We then use such a function in the `streaming_bulk` elasticsearch method together with the elasticsearch client and the index name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e2bd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_commits(limit):\n",
    "    counter = 0\n",
    "    Commits = []\n",
    "    for traverse_commit in esrepo.traverse_commits():\n",
    "        if counter >= limit:\n",
    "                break\n",
    "        for m in traverse_commit.modified_files:\n",
    "            if m.filename.endswith(file_type):\n",
    "                json_commit = json.dumps(jsonable_encoder(commit(hash=traverse_commit.hash, \n",
    "                msg=traverse_commit.msg, author_date=traverse_commit.author_date,  \n",
    "                author_name=traverse_commit.author.name, \n",
    "                author_email=traverse_commit.author.email,\n",
    "                commiter_name=traverse_commit.committer.name,\n",
    "                commiter_email=traverse_commit.committer.email,\n",
    "                deletions=traverse_commit.deletions,\n",
    "                insertions=traverse_commit.insertions,\n",
    "                lines=traverse_commit.lines,\n",
    "                files=traverse_commit.files,\n",
    "                modified_files=[modified_files(complexity=m.complexity, \n",
    "                                                nloc=m.nloc, \n",
    "                                                old_path=m.old_path, \n",
    "                                                new_path=m.new_path,\n",
    "                                                change_type=m.change_type,\n",
    "                                                filename=m.filename,\n",
    "                                                diff=m.diff,\n",
    "                                                added_lines=m.added_lines,\n",
    "                                                deleted_lines=m.deleted_lines,\n",
    "                                                source_code=m.source_code,\n",
    "                                                source_code_before=m.source_code_before,\n",
    "                                                methods=m.methods,\n",
    "                                                methods_before=m.methods_before,\n",
    "                                                changed_methods=m.changed_methods,\n",
    "                                                token_count=m.token_count\n",
    "                                                )])))\n",
    "                counter +=1\n",
    "                yield json_commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "57aa7337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 779/10000 [13:37<2:41:20,  1.05s/docs]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12476/1067763057.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprogress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"docs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msuccesses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstreaming_bulk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mesclient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myield_commits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprogress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msuccesses\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mok\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\fss\\lib\\site-packages\\elasticsearch\\helpers\\actions.py\u001b[0m in \u001b[0;36mstreaming_bulk\u001b[1;34m(client, actions, chunk_size, max_chunk_bytes, raise_on_error, expand_action_callback, raise_on_exception, max_retries, initial_backoff, max_backoff, yield_ok, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpand_action_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m     for bulk_data, bulk_actions in _chunk_actions(\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_chunk_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     ):\n",
      "\u001b[1;32m~\\.conda\\envs\\fss\\lib\\site-packages\\elasticsearch\\helpers\\actions.py\u001b[0m in \u001b[0;36m_chunk_actions\u001b[1;34m(actions, chunk_size, max_chunk_bytes, serializer)\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_chunk_bytes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_chunk_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     )\n\u001b[1;32m--> 155\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12476/2037273150.py\u001b[0m in \u001b[0;36myield_commits\u001b[1;34m(limit)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtraverse_commit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodified_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".go\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 json_commit = json.dumps(jsonable_encoder(commit(hash=traverse_commit.hash, \n",
      "\u001b[1;32m~\\.conda\\envs\\fss\\lib\\site-packages\\pydriller\\domain\\commit.py\u001b[0m in \u001b[0;36mmodified_files\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \"\"\"\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modifications\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modifications\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_modifications\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modifications\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\fss\\lib\\site-packages\\pydriller\\domain\\commit.py\u001b[0m in \u001b[0;36m_get_modifications\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparents\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m             \u001b[1;31m# the commit has a parent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 683\u001b[1;33m             diff_index = self._c_object.parents[0].diff(\n\u001b[0m\u001b[0;32m    684\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_object\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_patch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m             )\n",
      "\u001b[1;32m~\\.conda\\envs\\fss\\lib\\site-packages\\git\\diff.py\u001b[0m in \u001b[0;36mdiff\u001b[1;34m(self, other, paths, create_patch, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m                        \u001b[1;32mif\u001b[0m \u001b[0mcreate_patch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                        else Diff._index_from_raw_format)\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiff_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\fss\\lib\\site-packages\\git\\diff.py\u001b[0m in \u001b[0;36m_index_from_patch_format\u001b[1;34m(cls, repo, proc)\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[1;31m## FIXME: Here SLURPING raw, need to re-phrase header-regexes linewise.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[0mtext_list\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[0mhandle_process_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize_process\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_streams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[1;31m# for now, we have to bake the stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\fss\\lib\\site-packages\\git\\cmd.py\u001b[0m in \u001b[0;36mhandle_process_output\u001b[1;34m(process, stdout_handler, stderr_handler, finalizer, decode_streams, kill_after_timeout)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkill_after_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAutoInterrupt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\fss\\lib\\threading.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1053\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1054\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\fss\\lib\\threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# already determined that the C code is done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m             \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Increase the limit if you have more commits\n",
    "limit = 1000\n",
    "\n",
    "progress = tqdm.tqdm(unit=\"docs\", total=limit)\n",
    "successes = 0\n",
    "for ok, action in streaming_bulk(client=esclient, index=index_name, actions=yield_commits(limit), chunk_size=100):\n",
    "    progress.update(1)\n",
    "    successes += ok\n",
    "    \n",
    "print(\"Indexed %d/%d documents\" % (successes, limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0731e0d",
   "metadata": {},
   "source": [
    "## We can now use Kibana for analysis\n",
    "\n",
    "At this point we have imported a number of commits (together with all their data) equal to `limit`. We can now open localhost at port 5601 to analyze and visualize the data we imported with Kibana, the visualization engine of ElasticSearch. You may need to create an Index Pattern using the Kibaba UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b7180",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de19df9758f468667ac5d5f7f5860bc0ef58c4aac869d7090d154ea1440dbf9a"
  },
  "kernelspec": {
   "display_name": "FSS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
