{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb50353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydriller\n",
    "from pydriller import ModificationType\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import streaming_bulk\n",
    "import json\n",
    "from datetime import date, datetime\n",
    "import tqdm\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10600c15",
   "metadata": {},
   "source": [
    "## Create the elasticsearch and pydriller clients\n",
    "\n",
    "First we instantiate the elasticsearch client. If you have docker installed, you can use the following command to create a docker container with elasticsearch:\n",
    "```docker\n",
    "docker network create elastic\n",
    "docker pull docker.elastic.co/elasticsearch/elasticsearch:7.15.2\n",
    "docker run --name es01-test --net elastic -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.15.2\n",
    "\n",
    "```\n",
    "\n",
    "To run the Kibana server, you can use the following command:\n",
    "```docker\n",
    "docker pull docker.elastic.co/kibana/kibana:7.15.2\n",
    "docker run --name kib01-test --net elastic -p 127.0.0.1:5601:5601 -e \"ELASTICSEARCH_HOSTS=http://es01-test:9200\" docker.elastic.co/kibana/kibana:7.15.2\n",
    "\n",
    "```\n",
    "\n",
    "Without specifying parameters, it will look for an elasticsearch instance on localhost, port 9200 (default)\n",
    "\n",
    "We then specify the name of the index where we want to store our commit data. In this case, we will look at the go-ipfs git repository itself, so we call the index \"go-ipfs-commits\" but this is completely arbitrary.\n",
    "In case the index already exist and we want to wipe it to re-ingest the data we can run the delete command on the indeces.\n",
    "\n",
    "Finally, we create a pydriller repository object by specifing the location on the local file system where we cloned the desired repository, in our case \"C:\\Projects\\External-Examples\\go-ipfs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "298b1c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esclient = Elasticsearch()\n",
    "index_name = \"go-ipfs-commits\"\n",
    "start_tag =  'v0.5.0'\n",
    "end_tag = 'v0.10.0'\n",
    "file_type = '.go'\n",
    "\n",
    "\n",
    "esrepo = pydriller.Repository('C:\\Projects\\External-Examples\\go-ipfs', from_tag=start_tag, to_tag=end_tag)\n",
    "\n",
    "# in case we want to wipe the index and re-ingest the data\n",
    "# esclient.indices.delete(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc67bf",
   "metadata": {},
   "source": [
    "## Define the elasticsearch schema\n",
    "\n",
    "In case this is the first time we import the data, or if we deleted the index, the next step is to specify the schema of the elasticsearch index we use. Note that we can also import the data without specifying the schema, and elastic will infer the schema itself. However, elastic doesn't know how we want to analyze the data, so using the default schema inference is not recommended.\n",
    "\n",
    "In the schema, besides the standard data type mapping such as integer, float, and datetime, which are simply a 1:1 mapping with python, we have text and keyword. keyword should be used for strings that we want to analyze as they are (for example author name or email, commit hash); text is for strings where we want to apply full text search capabilities. In this case we can also specify which analyzer to use, to define which stemming rules to apply and stop-world to remove. We use the english analyzer for the commit text messages, which are the only human writte piece of information in the commit (excluding the actual diff). In the mapping for the commit message (field msg) we also set \"fielddata\" to be true, so that we can apply techniques normally available only for keyword fields to the commit message (for example to generate the tag cloud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dcb8078d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'go-ipfs-commits-complete'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esclient.indices.create(\n",
    "    index_name, \n",
    "    body={\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"author.name\": {\"type\": \"keyword\"},\n",
    "                \"author.email\": {\"type\": \"keyword\"},\n",
    "                \"committer.name\": {\"type\": \"keyword\"},\n",
    "                \"committer.email\": {\"type\": \"keyword\"},\n",
    "                \"deletions\": {\"type\": \"integer\"},\n",
    "                \"insertions\": {\"type\": \"integer\"},\n",
    "                \"lines\": {\"type\": \"integer\"},\n",
    "                \"files\": {\"type\": \"integer\"},\n",
    "                \n",
    "                \"author_timezone\": {\"type\": \"integer\"}, \n",
    "                \"committer_date\": {\"type\": \"date\"},\n",
    "                \"committer_timezone\": {\"type\": \"integer\"}, \n",
    "                \"parents\": {\"type\": \"keyword\"},\n",
    "\n",
    "                \"old_path\": {\"type\": \"keyword\"},\n",
    "                \"new_path\": {\"type\": \"keyword\"},\n",
    "                \"filename\": {\"type\": \"keyword\"},\n",
    "                \"change_type\": {\"type\": \"keyword\"},\n",
    "                \"diff\": {\"type\": \"text\"},\n",
    "                \"added_lines\": {\"type\": \"integer\"},\n",
    "                \"deleted_lines\": {\"type\": \"integer\"},\n",
    "                \"source_code\": {\"type\": \"text\"},\n",
    "                \"source_code_before\": {\"type\": \"text\"},\n",
    "                \"methods\": {\"type\": \"text\"},\n",
    "                \"methods_before\": {\"type\": \"text\"},\n",
    "                \"changed_methods\": {\"type\": \"text\"},\n",
    "                \"nloc\": {\"type\": \"integer\"},\n",
    "                \"complexity\": {\"type\": \"integer\"},\n",
    "                \"token_count\": {\"type\": \"integer\"},\n",
    "            }}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2d1e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class modified_files(BaseModel):\n",
    "    complexity: Optional[int]\n",
    "    nloc: Optional[int]\n",
    "    old_path: Optional[str]\n",
    "    new_path: Optional[str]\n",
    "    change_type: Optional[ModificationType]\n",
    "    filename: Optional[str]\n",
    "    diff: Optional[str]\n",
    "    added_lines: Optional[int]\n",
    "    deleted_lines: Optional[int]\n",
    "    source_code: Optional[str]\n",
    "    source_code_before: Optional[str]\n",
    "    methods: Optional[List]\n",
    "    methods_before: Optional[List]\n",
    "    changed_methods: Optional[List]\n",
    "    token_count: Optional[int]\n",
    "    \n",
    "\n",
    "\n",
    "class commit(BaseModel):\n",
    "    hash: str\n",
    "    msg: str\n",
    "    author_date: datetime\n",
    "    author_name: str\n",
    "    author_email: str\n",
    "    modified_files: List[modified_files]\n",
    "    commiter_name: str\n",
    "    commiter_email: str\n",
    "    deletions: int\n",
    "    insertions: int\n",
    "    lines: int\n",
    "    files: int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce46f0",
   "metadata": {},
   "source": [
    "## Importing the data in ElasticSearch\n",
    "\n",
    "### Converting Pydantic Pydriller objects into json\n",
    "\n",
    "Once we have an elasticsearch index ready to receive our data, we need a way to convert Pydanitc pydriller objects into json format, the one expected by elasticsearch (we could also use other format with elasticsearch pipelines and plugins, but json is the default, easiest, and preferred format).\n",
    "\n",
    "To convert python objects into json, the standard way is to use the json library with the json.dumps function. We use the Fastapi's json_encode to convert the python objects into json.\n",
    "\n",
    "### Bulk importing the data\n",
    "\n",
    "Now that we have the index, the schema, and a way to encode pydriller objects into json, we are ready to import the data in ElasticSearch. To do that, we use the bulk endpoints, which are way more efficient than the standard ingest endpoints to import large number of documents.\n",
    "\n",
    "We first define a function that given a number of commits to import (`limit`), traverses the git history, converts every commit into json and yields back the converted commit.\n",
    "\n",
    "We then use such a function in the `streaming_bulk` elasticsearch method together with the elasticsearch client and the index name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e2bd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_commits(limit):\n",
    "    counter = 0\n",
    "    Commits = []\n",
    "    for traverse_commit in esrepo.traverse_commits():\n",
    "        if counter >= limit:\n",
    "                break\n",
    "        for m in traverse_commit.modified_files:\n",
    "            if m.filename.endswith(file_type):\n",
    "                json_commit = json.dumps(jsonable_encoder(commit(hash=traverse_commit.hash, \n",
    "                msg=traverse_commit.msg, author_date=traverse_commit.author_date,  \n",
    "                author_name=traverse_commit.author.name, \n",
    "                author_email=traverse_commit.author.email,\n",
    "                commiter_name=traverse_commit.committer.name,\n",
    "                commiter_email=traverse_commit.committer.email,\n",
    "                deletions=traverse_commit.deletions,\n",
    "                insertions=traverse_commit.insertions,\n",
    "                lines=traverse_commit.lines,\n",
    "                files=traverse_commit.files,\n",
    "                modified_files=[modified_files(complexity=m.complexity, \n",
    "                                                nloc=m.nloc, \n",
    "                                                old_path=m.old_path, \n",
    "                                                new_path=m.new_path,\n",
    "                                                change_type=m.change_type,\n",
    "                                                filename=m.filename,\n",
    "                                                diff=m.diff,\n",
    "                                                added_lines=m.added_lines,\n",
    "                                                deleted_lines=m.deleted_lines,\n",
    "                                                source_code=m.source_code,\n",
    "                                                source_code_before=m.source_code_before,\n",
    "                                                methods=m.methods,\n",
    "                                                methods_before=m.methods_before,\n",
    "                                                changed_methods=m.changed_methods,\n",
    "                                                token_count=m.token_count\n",
    "                                                )])))\n",
    "                counter +=1\n",
    "                yield json_commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the limit if you have more commits\n",
    "limit = 1000\n",
    "\n",
    "progress = tqdm.tqdm(unit=\"docs\", total=limit)\n",
    "successes = 0\n",
    "for ok, action in streaming_bulk(client=esclient, index=index_name, actions=yield_commits(limit), chunk_size=100):\n",
    "    progress.update(1)\n",
    "    successes += ok\n",
    "    \n",
    "print(\"Indexed %d/%d documents\" % (successes, limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0731e0d",
   "metadata": {},
   "source": [
    "## We can now use Kibana for analysis\n",
    "\n",
    "At this point we have imported a number of commits (together with all their data) equal to `limit`. We can now open localhost at port 5601 to analyze and visualize the data we imported with Kibana, the visualization engine of ElasticSearch. You may need to create an Index Pattern using the Kibaba UI."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de19df9758f468667ac5d5f7f5860bc0ef58c4aac869d7090d154ea1440dbf9a"
  },
  "kernelspec": {
   "display_name": "FSS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
